{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Private 6th",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1l2H2Lcauzx9rz7hjHNgt9oZZmHJpMNzm",
      "authorship_tag": "ABX9TyOr2OfFMyHK+Tyrqa3/9Tcu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "65b6169e8a394c54b692bf073c293dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_03a6028b3eb4407b8745c3d6ad341537",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5114aa1cf7684b86b8172bd8fefe10a6",
              "IPY_MODEL_15c95b26eeca417bb615fdd4d2011175",
              "IPY_MODEL_758576999865402b9dac1dff50355d95"
            ]
          }
        },
        "03a6028b3eb4407b8745c3d6ad341537": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5114aa1cf7684b86b8172bd8fefe10a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_53bc418340eb4512af707b659b9eda88",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3505b7e638bc409a8258e7484cdd6de3"
          }
        },
        "15c95b26eeca417bb615fdd4d2011175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_19eccd4cb1ce4f7c9ec9bb1a153bea1d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 61,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 61,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7158a302da24c26af382ad4ed002463"
          }
        },
        "758576999865402b9dac1dff50355d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1312eb0bd72648bb9c58bc38cf51abc0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 61.0/61.0 [00:00&lt;00:00, 2.33kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c2354c486184ab0996f8fea9c19b27f"
          }
        },
        "53bc418340eb4512af707b659b9eda88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3505b7e638bc409a8258e7484cdd6de3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19eccd4cb1ce4f7c9ec9bb1a153bea1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7158a302da24c26af382ad4ed002463": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1312eb0bd72648bb9c58bc38cf51abc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c2354c486184ab0996f8fea9c19b27f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f12e4a1c28dd41d994af09f77271d7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a4c2b323036e496592bd8b17e9d6f40e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_05fe63c9247b45a4b8d7bd0759d6049e",
              "IPY_MODEL_00ca62bbe9594824bee335062acd16c2",
              "IPY_MODEL_c9a57d3691f1414ebf151cee415e1f42"
            ]
          }
        },
        "a4c2b323036e496592bd8b17e9d6f40e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "05fe63c9247b45a4b8d7bd0759d6049e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_52356edf065b4d8c8bd7766fca8ad10d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e9cdd5eeb2174aedb9760660ab6030a0"
          }
        },
        "00ca62bbe9594824bee335062acd16c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_234ce57c4bca4ac6a93a50b68131487a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 467,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 467,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce1afec0f1094f6385605f40c744297b"
          }
        },
        "c9a57d3691f1414ebf151cee415e1f42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1bebe160a4084a84af249968bf7ea8f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 467/467 [00:00&lt;00:00, 18.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03a7a949bab946e5bb727489d3d53c24"
          }
        },
        "52356edf065b4d8c8bd7766fca8ad10d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e9cdd5eeb2174aedb9760660ab6030a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "234ce57c4bca4ac6a93a50b68131487a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce1afec0f1094f6385605f40c744297b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1bebe160a4084a84af249968bf7ea8f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03a7a949bab946e5bb727489d3d53c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fde6feb1515846928c8d6633461f212e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_35b45c53e70f43de973a22fe0669c317",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c4ffc22ee094a1186d98b164ad114f9",
              "IPY_MODEL_04a4614a63ee4cdebb4c24bbf861c6da",
              "IPY_MODEL_9762ee2ca9694d1ab1d5bd4bc7ba895e"
            ]
          }
        },
        "35b45c53e70f43de973a22fe0669c317": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c4ffc22ee094a1186d98b164ad114f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb23d10154a14fc996232660c73fb4d8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_435dd48ab57943409b120d65ffa148d7"
          }
        },
        "04a4614a63ee4cdebb4c24bbf861c6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7689a0fff5c14eb9af2253cfbb2a12b3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 263326,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 263326,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ccf483ef29044c0a012580cdac8b1f0"
          }
        },
        "9762ee2ca9694d1ab1d5bd4bc7ba895e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59dc052cb80641e2954845e3dac2358d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 257k/257k [00:00&lt;00:00, 927kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f4695c4ec3b451d80c94c3a069a76e2"
          }
        },
        "eb23d10154a14fc996232660c73fb4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "435dd48ab57943409b120d65ffa148d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7689a0fff5c14eb9af2253cfbb2a12b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ccf483ef29044c0a012580cdac8b1f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59dc052cb80641e2954845e3dac2358d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f4695c4ec3b451d80c94c3a069a76e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "138fdb3856f1458699af82082e258dad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_76f687b8bf1c4065a0ca3205e30f886e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b5e41a8527434e59bb490540ce8927c4",
              "IPY_MODEL_edc78a70b7f0410eac70b8c971c48192",
              "IPY_MODEL_7f4d74e962154917a0e1d18529fce0dc"
            ]
          }
        },
        "76f687b8bf1c4065a0ca3205e30f886e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b5e41a8527434e59bb490540ce8927c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a41b657259f44b47baccb2abbcba0ed6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c13da380f1d40d4ba70c6a298a5f7a0"
          }
        },
        "edc78a70b7f0410eac70b8c971c48192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b59ecf486d56463ab772573bad819989",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 451741507,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 451741507,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_068f21d128f3454a8a100a836d579a80"
          }
        },
        "7f4d74e962154917a0e1d18529fce0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a3cbb404b467439a884cab7d70919d08",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 431M/431M [00:10&lt;00:00, 57.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a525b39856154e5b93be805cf32d5c88"
          }
        },
        "a41b657259f44b47baccb2abbcba0ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c13da380f1d40d4ba70c6a298a5f7a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b59ecf486d56463ab772573bad819989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "068f21d128f3454a8a100a836d579a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a3cbb404b467439a884cab7d70919d08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a525b39856154e5b93be805cf32d5c88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ssunggun2/2022_dacon_NLI/blob/main/Private_6th.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GAv9hYRQTcOp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3083b962-b274-4979-d660-3cc22478a677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google import colab\n",
        "colab.drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. 모듈 설치 및 import"
      ],
      "metadata": {
        "id": "0FPIQ4se8Cx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets\n",
        "!pip install selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install googletrans==4.0.0-rc1\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPUTk1x271S-",
        "outputId": "3d1f7031-91cb-47a2-b77b-71494a0d71fe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-1.18.3-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 53.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.0 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 75.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 53.7 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 56.7 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 97.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyyaml, fsspec, aiohttp, xxhash, tokenizers, sacremoses, huggingface-hub, transformers, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.3 frozenlist-1.3.0 fsspec-2022.2.0 huggingface-hub-0.4.0 multidict-6.0.2 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0 xxhash-3.0.0 yarl-1.7.2\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.1.2-py3-none-any.whl (963 kB)\n",
            "\u001b[K     |████████████████████████████████| 963 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 65.2 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (3.10.0.2)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.8 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.1 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.2 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.8 wsproto-1.1.0\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [76.8 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [931 kB]\n",
            "Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:16 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,474 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [806 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,036 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,596 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,827 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [840 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,252 kB]\n",
            "Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [936 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [42.8 kB]\n",
            "Fetched 15.1 MB in 4s (3,544 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-470\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 75 not upgraded.\n",
            "Need to get 88.3 MB of archives.\n",
            "After this operation, 294 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 99.0.4844.51-0ubuntu0.18.04.1 [1,143 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 99.0.4844.51-0ubuntu0.18.04.1 [77.6 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 99.0.4844.51-0ubuntu0.18.04.1 [4,388 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 99.0.4844.51-0ubuntu0.18.04.1 [5,092 kB]\n",
            "Fetched 88.3 MB in 6s (16.0 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155320 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_99.0.4844.51-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_99.0.4844.51-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (99.0.4844.51-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2021.10.8)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.2.0)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2021.12.1-py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 21.2 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17416 sha256=500b6ad0347f2f3dd7e139de55ce1dd1ad44ab4eb433c672a3956974341190e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/34/00/4fe71786ea6d12314b29037620c36d857e5d104ac2748bf82a\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "  Attempting uninstall: h11\n",
            "    Found existing installation: h11 0.13.0\n",
            "    Uninstalling h11-0.13.0:\n",
            "      Successfully uninstalled h11-0.13.0\n",
            "Successfully installed googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2021.12.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, TFAutoModel, AdamWeightDecay,  AutoModelForSequenceClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "import selenium\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n"
      ],
      "metadata": {
        "id": "bJZGFMlq8iKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "089efccc-320c-483b-8a9d-050f43d88998"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.26.8) or chardet (3.0.4) doesn't match a supported version!\n",
            "  RequestsDependencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "seed"
      ],
      "metadata": {
        "id": "qt3yTexYX0_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int = 2022):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything()\n"
      ],
      "metadata": {
        "id": "K-IE6-vl83wZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. DATA LOAD"
      ],
      "metadata": {
        "id": "5uIz5oCOZz8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-1 기존 데이터, dev 추가 데이터 : train_total for model1"
      ],
      "metadata": {
        "id": "sRHcJ8yWZ5hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "ut0-Oz9tcZ5b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_dir = '/content/drive/MyDrive/화요일 스터디 폴더/dacon'\n",
        "zip_name = 'open.zip'\n",
        "zip_fPath = os.path.join(zip_dir, zip_name)\n",
        "\n",
        "with ZipFile(zip_fPath, 'r') as zip1:\n",
        "    zip1.printdir()\n",
        "    zip1.extractall() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOS7OXBscmbj",
        "outputId": "2b69db24-209e-47c3-be83-8b538825afa3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Name                                             Modified             Size\n",
            "open/sample_submission.csv                     2022-01-25 15:19:44        18894\n",
            "open/test_data.csv                             2022-01-25 15:19:46       320363\n",
            "open/train_data.csv                            2022-01-25 17:30:34      4855176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/content/open/train_data.csv')\n",
        "test_df = pd.read_csv('/content/open/test_data.csv')\n",
        "submission_df = pd.read_csv('/content/open/sample_submission.csv')\n",
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "3InnKyzUjW3G",
        "outputId": "8f385151-c0b8-4735-9b22-13364dce15ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-638941fe-f337-41e6-964d-970af8ed5426\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...</td>\n",
              "      <td>씨름의 여자들의 놀이이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...</td>\n",
              "      <td>자작극을 벌인 이는 3명이다.</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.</td>\n",
              "      <td>예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...</td>\n",
              "      <td>원주민들은 종합대책에 만족했다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...</td>\n",
              "      <td>이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-638941fe-f337-41e6-964d-970af8ed5426')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-638941fe-f337-41e6-964d-970af8ed5426 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-638941fe-f337-41e6-964d-970af8ed5426');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index                                            premise  \\\n",
              "0      0  씨름은 상고시대로부터 전해져 내려오는 남자들의 대표적인 놀이로서, 소년이나 장정들이...   \n",
              "1      1  삼성은 자작극을 벌인 2명에게 형사 고소 등의 법적 대응을 검토 중이라고 하였으나,...   \n",
              "2      2                    이를 위해 예측적 범죄예방 시스템을 구축하고 고도화한다.   \n",
              "3      3  광주광역시가 재개발 정비사업 원주민들에 대한 종합대책을 마련하는 등 원주민 보호에 ...   \n",
              "4      4  진정 소비자와 직원들에게 사랑 받는 기업으로 오래 지속되고 싶으면, 이런 상황에서는...   \n",
              "\n",
              "                                hypothesis          label  \n",
              "0                           씨름의 여자들의 놀이이다.  contradiction  \n",
              "1                         자작극을 벌인 이는 3명이다.  contradiction  \n",
              "2  예측적 범죄예방 시스템 구축하고 고도화하는 것은 목적이 있기 때문이다.     entailment  \n",
              "3                        원주민들은 종합대책에 만족했다.        neutral  \n",
              "4       이런 상황에서 책임 있는 모습을 보여주는 기업은 아주 드물다.        neutral  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "추가 데이터는 airbnb 데이터, wikinews"
      ],
      "metadata": {
        "id": "MUxRFKhejF0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = zip_dir + \"/data/translate/\" #번역본의 위치\n",
        "train_path2 = zip_dir + \"/klue-nli-v1.1/klue-nli-v1.1_dev.json\""
      ],
      "metadata": {
        "id": "8bf5uS6hctgd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "jbBOzCkGisG6",
        "outputId": "a3737491-72a4-4f25-af31-376c9f8dbb38"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/화요일 스터디 폴더/dacon/klue-nli-v1.1/klue-nli-v1.1_dev.json'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_additional = pd.read_json(train_path2)\n",
        "train_additional.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "UdFdaymfiVXu",
        "outputId": "0abb8fe9-b5d7-4f74-d4d4-0e5acb29d71f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f9072c50-1e81-475f-8f04-1069e1780f25\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>guid</th>\n",
              "      <th>source</th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>gold_label</th>\n",
              "      <th>author</th>\n",
              "      <th>label2</th>\n",
              "      <th>label3</th>\n",
              "      <th>label4</th>\n",
              "      <th>label5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klue-nli-v1_dev_00000</td>\n",
              "      <td>airbnb</td>\n",
              "      <td>흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.</td>\n",
              "      <td>어떤 방에서도 흡연은 금지됩니다.</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>klue-nli-v1_dev_00001</td>\n",
              "      <td>airbnb</td>\n",
              "      <td>10명이 함께 사용하기 불편함없이 만족했다.</td>\n",
              "      <td>10명이 함께 사용하기 불편함이 많았다.</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "      <td>contradiction</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>klue-nli-v1_dev_00002</td>\n",
              "      <td>airbnb</td>\n",
              "      <td>10명이 함께 사용하기 불편함없이 만족했다.</td>\n",
              "      <td>성인 10명이 함께 사용하기 불편함없이 없었다.</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>neutral</td>\n",
              "      <td>entailment</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>klue-nli-v1_dev_00003</td>\n",
              "      <td>airbnb</td>\n",
              "      <td>10명이 함께 사용하기 불편함없이 만족했다.</td>\n",
              "      <td>10명이 함께 사용하기에 만족스러웠다.</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>klue-nli-v1_dev_00004</td>\n",
              "      <td>airbnb</td>\n",
              "      <td>10층에 건물사람들만 이용하는 수영장과 썬베드들이 있구요.</td>\n",
              "      <td>건물사람들은 수영장과 썬베드를 이용할 수 있습니다.</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "      <td>entailment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9072c50-1e81-475f-8f04-1069e1780f25')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9072c50-1e81-475f-8f04-1069e1780f25 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9072c50-1e81-475f-8f04-1069e1780f25');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    guid  source                              premise  \\\n",
              "0  klue-nli-v1_dev_00000  airbnb  흡연자분들은 발코니가 있는 방이면 발코니에서 흡연이 가능합니다.   \n",
              "1  klue-nli-v1_dev_00001  airbnb             10명이 함께 사용하기 불편함없이 만족했다.   \n",
              "2  klue-nli-v1_dev_00002  airbnb             10명이 함께 사용하기 불편함없이 만족했다.   \n",
              "3  klue-nli-v1_dev_00003  airbnb             10명이 함께 사용하기 불편함없이 만족했다.   \n",
              "4  klue-nli-v1_dev_00004  airbnb     10층에 건물사람들만 이용하는 수영장과 썬베드들이 있구요.   \n",
              "\n",
              "                     hypothesis     gold_label         author         label2  \\\n",
              "0            어떤 방에서도 흡연은 금지됩니다.  contradiction  contradiction  contradiction   \n",
              "1        10명이 함께 사용하기 불편함이 많았다.  contradiction  contradiction  contradiction   \n",
              "2    성인 10명이 함께 사용하기 불편함없이 없었다.        neutral        neutral        neutral   \n",
              "3         10명이 함께 사용하기에 만족스러웠다.     entailment     entailment     entailment   \n",
              "4  건물사람들은 수영장과 썬베드를 이용할 수 있습니다.     entailment     entailment     entailment   \n",
              "\n",
              "          label3         label4         label5  \n",
              "0  contradiction  contradiction  contradiction  \n",
              "1  contradiction  contradiction  contradiction  \n",
              "2        neutral     entailment        neutral  \n",
              "3     entailment     entailment     entailment  \n",
              "4     entailment     entailment     entailment  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_additional['index'] = train_additional.index\n",
        "train_additional = train_additional[['index','premise','hypothesis','gold_label']]\n",
        "train_additional.columns = ['index','premise','hypothesis','label'] # <= concate 위해서"
      ],
      "metadata": {
        "id": "tvflZzBDhn9n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_total = pd.concat([train_df,train_additional])\n",
        "train_total.drop('index', axis = 1,inplace = True)"
      ],
      "metadata": {
        "id": "TNOc8wvUjTtB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-2 번역 데이터 준비"
      ],
      "metadata": {
        "id": "xdAuEJqskO1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Premise KOR > ENG (using PAPAGO, Googletrans)\n",
        "papago 크롤링은 뉴스 토픽 분류 AI 경진대회 최종 3th Kerry님의 코드를 참고했습니다 🙇"
      ],
      "metadata": {
        "id": "A5fVD-u1oUzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from selenium import webdriver\n",
        "# 코랩에서 크롬 웹드라이버 돌릴 수 있게\n",
        "def chrome_setting():\n",
        "  chrome_options = webdriver.ChromeOptions()\n",
        "  chrome_options.add_argument('--headless')\n",
        "  chrome_options.add_argument('--no-sandbox')\n",
        "  chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome('chromedriver', options=chrome_options)\n",
        "  return driver\n",
        "\n",
        "driver = chrome_setting()"
      ],
      "metadata": {
        "id": "JRJjO4G_nX-o"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df['premise'] = train_df['premise'].str.replace('%', '퍼센트')\n",
        "# train_df['hypothesis'] = train_df['hypothesis'].str.replace('&', ' 그리고 ')\n",
        "# train_additional['premise'] = train_additional['premise'].str.replace('%', '퍼센트')\n",
        "# train_additional['hypothesis'] = train_additional['hypothesis'].str.replace('&', ' 그리고 ')\n",
        "# #파파고 크롤링\n",
        "# def kor_to_trans(text_data, trans_lang,start_index,final_index):\n",
        "\n",
        "#   target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
        "\n",
        "#   for i in tqdm(range(start_index,final_index)): \n",
        "    \n",
        "#     if (i!=0)&(i%99==0):\n",
        "#       driver.implicitly_wait(2)\n",
        "#       print('{}th : '.format(i), backtrans)\n",
        "#       np.save(data_path+'p_kor_to_eng_train_{}_{}.npy'.format(start_index,final_index),trans_list)\n",
        "    \n",
        "#     try:\n",
        "#       driver.get('https://papago.naver.com/?sk=ko&tk='+trans_lang+'&st='+text_data[i])\n",
        "#       time.sleep(1.5)\n",
        "#       element=WebDriverWait(driver, 10).until(target_present)\n",
        "#       time.sleep(0.1)\n",
        "#       backtrans = element.text \n",
        "\n",
        "#       if (backtrans=='')|(backtrans==' '):\n",
        "#         element=WebDriverWait(driver, 10).until(target_present)\n",
        "#         backtrans = element.text \n",
        "#         trans_list.append(backtrans)\n",
        "#       else:\n",
        "#         trans_list.append(backtrans)\n",
        "    \n",
        "#     except:\n",
        "#       trans_list.append('')\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',0,2500)\n",
        "# np.save(data_path+'p_kor_to_eng_train_0_2500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',2500,5000)\n",
        "# np.save(data_path+'p_kor_to_eng_train_2500_5000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',5000,7500)\n",
        "# np.save(data_path+'p_kor_to_eng_train_5000_7500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',7500,10000)\n",
        "# np.save(data_path+'p_kor_to_eng_train_7500_10000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',10000,12500)\n",
        "# np.save(data_path+'p_kor_to_eng_train_10000_12500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',12500,15000)\n",
        "# np.save(data_path+'p_kor_to_eng_train_12500_15000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',15000,17500)\n",
        "# np.save(data_path+'p_kor_to_eng_train_15000_17500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',17500,20000)\n",
        "# np.save(data_path+'p_kor_to_eng_train_17500_20000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',20000,22500)\n",
        "# np.save(data_path+'p_kor_to_eng_train_20000_22500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# kor_to_trans(train_df['premise'], 'en',22500,len(train_df))\n",
        "# np.save(data_path+'p_kor_to_eng_train_22500_all.npy',trans_list) #24998개 번역\n",
        "# eng_data0 = np.load(data_path+'p_kor_to_eng_train_0_2500.npy')\n",
        "# eng_data1 = np.load(data_path+'p_kor_to_eng_train_2500_5000.npy')\n",
        "# eng_data2 = np.load(data_path+'p_kor_to_eng_train_5000_7500.npy')\n",
        "# eng_data3 = np.load(data_path+'p_kor_to_eng_train_7500_10000.npy')\n",
        "# eng_data4 = np.load(data_path+'p_kor_to_eng_train_10000_12500.npy')\n",
        "# eng_data5 = np.load(data_path+'p_kor_to_eng_train_12500_15000.npy')\n",
        "# eng_data6 = np.load(data_path+'p_kor_to_eng_train_15000_17500.npy')\n",
        "# eng_data7 = np.load(data_path+'p_kor_to_eng_train_17500_20000.npy')\n",
        "# eng_data8 = np.load(data_path+'p_kor_to_eng_train_20000_22500.npy')\n",
        "# eng_data9 = np.load(data_path+'p_kor_to_eng_train_22500_all.npy')\n",
        "\n",
        "# eng_data=[*eng_data0,*eng_data1,*eng_data2,*eng_data3,*eng_data4,*eng_data5,*eng_data6,*eng_data7,*eng_data8,*eng_data9]\n",
        "# eng_data=pd.DataFrame(eng_data,columns=['eng_premise'])\n",
        "# back_train=pd.concat([train_df,eng_data],axis=1)\n",
        "# no_back_train = back_train[(back_train['eng_premise']=='')]\n",
        "# #Googletrans \n",
        "\n",
        "# def translate_google_en(df, col, lang='en'):\n",
        " \n",
        "#   translator = Translator()\n",
        "#   df['eng'] = [translator.translate(i, src='ko', dest=lang).text for i in df[col]]   # 한영 변환을 새로운 칼럼 'eng'에 담습니다\n",
        "   \n",
        "#   tmp1 = df.drop(['eng'], axis=1)                                                    # 2개의 temporary 데이터프레임을 생성해 원본과 한영번역 데이터프레임을 합칩니다\n",
        "#   tmp2 = df.drop([col], axis=1)\n",
        "#   tmp2.rename(columns={'eng':'data'}, inplace=True)\n",
        "#   result = pd.concat([tmp1,tmp2], ignore_index=True)\n",
        "#   result = result.drop_duplicates()\n",
        "#   return result    \n",
        "# back_train2 = translate_google_en(no_back_train, 'premise')\n",
        "# back_train2 = back_train2[['index', 'premise', 'hypothesis', 'label', 'data' ]][len(no_back_train):]\n",
        "# back_train2 = back_train2.reset_index(drop=True)\n",
        "# # 인덱스 순서대로 삽입\n",
        "# for i,j in zip(back_train2['index'], range(len(no_back_train))) :\n",
        "#   back_train['eng_premise'].iloc[i] = back_train2['data'].iloc[j]\n",
        "# back_train.to_csv(data_path + \"premise_translate_eng.csv\", index=False) \n",
        "# Premise ENG > KOR\n",
        "# # 영한 파파고 번역기 크롤링\n",
        "# def trans_to_kor(transed_list, transed_lang,start_index,final_index): \n",
        "  \n",
        "#   target_present = EC.presence_of_element_located((By.XPATH, '//*[@id=\"txtTarget\"]'))\n",
        "\n",
        "#   for i in tqdm(range(start_index,final_index)): \n",
        "    \n",
        "#     if (i!=0)&(i%99==0):\n",
        "#       time.sleep(1.5)\n",
        "#       print('{}th : '.format(i), backtrans)\n",
        "#       np.save(data_path+'premise_eng_to_kor_train_{}_{}.npy'.format(start_index,final_index),trans_list)\n",
        "    \n",
        "#     try:\n",
        "#       driver.get('https://papago.naver.com/?sk=en&tk='+transed_lang+'&st='+transed_list[i])\n",
        "#       time.sleep(2)\n",
        "#       element=WebDriverWait(driver, 10).until(target_present)\n",
        "#       time.sleep(0.2)\n",
        "#       backtrans = element.text \n",
        "\n",
        "#       if (backtrans=='')|(backtrans==' '):\n",
        "#         element=WebDriverWait(driver, 10).until(target_present)\n",
        "#         backtrans = element.text \n",
        "#         trans_list.append(backtrans)\n",
        "#       else:\n",
        "#         trans_list.append(backtrans)\n",
        "    \n",
        "#     except:\n",
        "#       trans_list.append('')\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',0,2500)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_0_2500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',2500,5000)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_2500_5000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',5000,7500)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_5000_7500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',7500,10000)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_7500_10000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',10000,12500)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_10000_12500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',12500,15000)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_12500_15000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',15000,17500)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_15000_17500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',17500,20000)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_17500_20000.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',20000,22500)\n",
        "# np.save(data_path+'premise_eng_to_kor_train_20000_22500.npy',trans_list)\n",
        "\n",
        "# trans_list=[]\n",
        "# trans_to_kor(back_train['eng_premise'], 'ko',22500,len(train_df))\n",
        "# np.save(data_path+'premise_eng_to_kor_train_22500_all.npy',trans_list)\n",
        "# back0 = np.load(data_path+'premise_eng_to_kor_train_0_2500.npy')\n",
        "# back1 = np.load(data_path+'premise_eng_to_kor_train_2500_5000.npy')\n",
        "# back2 = np.load(data_path+'premise_eng_to_kor_train_5000_7500.npy')\n",
        "# back3 = np.load(data_path+'premise_eng_to_kor_train_7500_10000.npy')\n",
        "# back4 = np.load(data_path+'premise_eng_to_kor_train_10000_12500.npy')\n",
        "# back5 = np.load(data_path+'premise_eng_to_kor_train_12500_15000.npy')\n",
        "# back6 = np.load(data_path+'premise_eng_to_kor_train_15000_17500.npy')\n",
        "# back7 = np.load(data_path+'premise_eng_to_kor_train_17500_20000.npy')\n",
        "# back8 = np.load(data_path+'premise_eng_to_kor_train_20000_22500.npy')\n",
        "# back9 = np.load(data_path+'premise_eng_to_kor_train_22500_all.npy')\n",
        "\n",
        "# back_train_fin=[*back0,*back1,*back2,*back3,*back4,*back5,*back6,*back7,*back8,*back9]\n",
        "# back_train_fin=pd.DataFrame(back_train_fin,columns=['kor_premise'])\n",
        "# back_train_fin=pd.concat([train_df,back_train_fin],axis=1)\n",
        "# # 0.2내로 너무 짧은 번역\n",
        "# for i in back_train_fin['kor_premise'].map(len) :\n",
        "#   no_back_train = back_train_fin[(back_train_fin['kor_premise'].map(len) < i * 0.2)] \n",
        "# # Googletrans\n",
        "# def translate_google_ko(df, col, lang='ko'):\n",
        " \n",
        "#   translator = Translator()\n",
        "#   df['eng'] = [translator.translate(i, src='en', dest=lang).text for i in df[col]]   # 한영 변환을 새로운 칼럼 'eng'에 담습니다\n",
        "   \n",
        "#   tmp1 = df.drop(['eng'], axis=1)                                                    # 2개의 temporary 데이터프레임을 생성해 원본과 한영번역 데이터프레임을 합칩니다\n",
        "#   tmp2 = df.drop([col], axis=1)\n",
        "#   tmp2.rename(columns={'eng':'data2'}, inplace=True)\n",
        "#   result = pd.concat([tmp1,tmp2], ignore_index=True)\n",
        "#   result = result.drop_duplicates()\n",
        "#   return result    \n",
        "# back_train2 = translate_google_en(no_back_train, 'premise')\n",
        "# back_train2 = back_train2[['index', 'premise', 'hypothesis', 'label', 'kor_premise','data' ]][len(no_back_train):]\n",
        "# back_train2 = back_train2.reset_index(drop=True)\n",
        "# back_train3 = translate_google_ko(back_train2, 'data')\n",
        "# back_train3 = back_train3[['index', 'premise', 'hypothesis', 'label', 'kor_premise','data2' ]][len(no_back_train):]\n",
        "# back_train3 = back_train3.reset_index(drop=True)\n",
        "# for i,j in zip(back_train3['index'], range(len(no_back_train))) :\n",
        "#   back_train_fin['kor_premise'].iloc[i] = back_train3['data2'].iloc[j]\n",
        "# back_train_fin.to_csv(data_path + \"premise_translate_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "rcI54RAGk5Gw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 토크나이징 및 모델 학습"
      ],
      "metadata": {
        "id": "3Z_dHDg1wNH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(sent_list1, sent_list2, max_seq_len, tokenizer):\n",
        "\n",
        "    input_ids, attention_masks, token_type_ids = [], [], []\n",
        "\n",
        "    for sent1, sent2 in tqdm(zip(sent_list1, sent_list2), total=len(sent_list1)):\n",
        "        encoding_result = tokenizer.encode_plus(sent1, sent2, max_length = max_seq_len, pad_to_max_length = True)\n",
        "\n",
        "        input_ids.append(encoding_result['input_ids'])\n",
        "        attention_masks.append(encoding_result['attention_mask'])\n",
        "        token_type_ids.append(encoding_result['token_type_ids'])\n",
        "\n",
        "    input_ids = np.array(input_ids, dtype= int)\n",
        "    attention_masks = np.array(attention_masks, dtype=int)\n",
        "    token_type_ids = np.array(token_type_ids, dtype= int)\n",
        "\n",
        "    return (input_ids, attention_masks, token_type_ids)"
      ],
      "metadata": {
        "id": "lALgL1pvwQd8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NLIClassifier(Model):\n",
        "    def __init__(self, model_name):\n",
        "        super(NLIClassifier, self).__init__()\n",
        "        self.bert = TFAutoModel.from_pretrained(model_name, num_labels=3, from_pt=True)\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
        "        self.classifier = Dense(3, kernel_initializer=TruncatedNormal(0.02),activation='softmax') #가중치 초기화, 활성화함수\n",
        "\n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        input_ids, attention_mask, token_type_ids=inputs\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1]\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        prediction = self.classifier(pooled_output)\n",
        "\n",
        "        return prediction"
      ],
      "metadata": {
        "id": "RK7KVzGeykhY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fold_training(train, test_df, model_name):\n",
        "    cv = StratifiedKFold(n_splits=5, random_state = 2022, shuffle=True)\n",
        "    cv_pred = []\n",
        "\n",
        "    for idx, (train_idx, val_idx) in enumerate(cv.split(train[[\"premise\",\"hypothesis\"]], pd.DataFrame(train['label']))):\n",
        "        X_train, X_val = train[[\"premise\", \"hypothesis\"]].iloc[train_idx], train[[\"premise\", \"hypothesis\"]].iloc[val_idx]\n",
        "        y_train, y_val = pd.DataFrame(train['label']).iloc[train_idx], pd.DataFrame(train['label']).iloc[val_idx]\n",
        "        \n",
        "        train_fold = pd.concat([X_train, y_train], axis=1)\n",
        "        valid_fold = pd.concat([X_val, y_val], axis =1)\n",
        "\n",
        "        train_fold['premise'] = train_fold['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣0-9]', '')\n",
        "        train_fold['hypothesis'] = train_fold['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣0-9]', '')\n",
        "        valid_fold['premise'] = valid_fold['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣0-9]', '')\n",
        "        valid_fold['hypothesis'] = valid_fold['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣0-9]', '')\n",
        "\n",
        "        tokenizer= AutoTokenizer.from_pretrained(model_name)\n",
        "        max_seq_len = 128\n",
        "\n",
        "        X_train = convert_examples_to_features(train_fold['premise'], train_fold['hypothesis'], max_seq_len=max_seq_len, tokenizer = tokenizer)\n",
        "        X_valid = convert_examples_to_features(valid_fold['premise'], valid_fold['hypothesis'], max_seq_len = max_seq_len, tokenizer = tokenizer)\n",
        "        X_test = convert_examples_to_features(test_df['premise'], test_df['hypothesis'], max_seq_len = max_seq_len, tokenizer = tokenizer)\n",
        "\n",
        "        train_label = train_fold['label'].tolist()\n",
        "        val_label = valid_fold['label'].tolist()\n",
        "        test_label = test_df['label'].tolist()\n",
        "        idx_encode = LabelEncoder()\n",
        "        idx_encode.fit(train_label)\n",
        "\n",
        "        y_train = idx_encode.transform(train_label) # 주어진 고유한 정수로 변환\n",
        "        y_val = idx_encode.transform(val_label) # 고유한 정수로 변환\n",
        "\n",
        "        label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))   #원래 idx와 transform한 idx 딕셔너리로   # classes  => 동일 idx 생략 및 정리\n",
        "        idx_label = {value:key for key, value in label_idx.items()}\n",
        "\n",
        "        model = NLIClassifier(model_name)\n",
        "\n",
        "        optimizer = AdamWeightDecay(1e-5, weight_decay_rate=1e-4)   # 과적합 방지\n",
        "        loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "        model.compile(optimizer = optimizer, loss = loss, metrics = [metric])\n",
        "        \n",
        "\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor = 'val_accuracy',\n",
        "            min_delta = 0.001,\n",
        "            mode = 'max',\n",
        "            patience =2,\n",
        "        )\n",
        "\n",
        "        model.fit(\n",
        "            X_train, y_train, epochs = 3, batch_size = 8, validation_data = (X_valid, y_val),\n",
        "            callbacks = [early_stopping]\n",
        "            )\n",
        "        \n",
        "        pred = model.predict(X_test)\n",
        "        pred_df = pd.DataFrame(pred)\n",
        "        cv_pred.append(pred_df)\n",
        "        print('fold{} training finished!'.format(idx+1))\n",
        "\n",
        "    \n",
        "    return cv_pred"
      ],
      "metadata": {
        "id": "RZWI95oF1ezP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training(train, test_df, model_name):\n",
        "  train, valid = train_test_split(train, test_size=0.2, shuffle=True, stratify=train['label'], random_state = 2022)\n",
        "\n",
        "  train['premise'] = train['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
        "  train['hypothesis'] = train['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
        "  valid['premise'] = valid['premise'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
        "  valid['hypothesis'] = valid['hypothesis'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]', '')\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  max_seq_len = 128\n",
        "\n",
        "  X_train = convert_examples_to_features(train['premise'], train['hypothesis'],  max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
        "  X_valid = convert_examples_to_features(valid['premise'], valid['hypothesis'], max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
        "  X_test = convert_examples_to_features(test_df['premise'], test_df['hypothesis'],max_seq_len=max_seq_len, tokenizer=tokenizer)\n",
        "\n",
        "  train_label = train['label'].tolist()\n",
        "  val_label = valid['label'].tolist()\n",
        "  test_label = test_df['label'].tolist()\n",
        "  idx_encode = LabelEncoder()\n",
        "  idx_encode.fit(train_label)\n",
        "\n",
        "  y_train = idx_encode.transform(train_label) # 주어진 고유한 정수로 변환\n",
        "  y_val = idx_encode.transform(val_label) # 고유한 정수로 변환\n",
        "\n",
        "  label_idx = dict(zip(list(idx_encode.classes_), idx_encode.transform(list(idx_encode.classes_))))\n",
        "  idx_label = {value: key for key, value in label_idx.items()}\n",
        "\n",
        "  model = NLIClassifier(model_name)\n",
        "\n",
        "  optimizer = AdamWeightDecay(1e-5, weight_decay_rate=1e-4)  #과적합 방지\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "  early_stopping = EarlyStopping(\n",
        "      monitor=\"val_accuracy\", \n",
        "      min_delta=0.001,\n",
        "      mode = 'max',\n",
        "      patience=2)\n",
        "    \n",
        "  model.fit(\n",
        "      X_train, y_train, epochs=3, batch_size = 8, validation_data=(X_valid, y_val),\n",
        "      )#callbacks = [early_stopping])\n",
        "    \n",
        "  pred = model.predict(X_test)\n",
        "  pred_df = pd.DataFrame(pred)\n",
        "  return pred_df"
      ],
      "metadata": {
        "id": "-IhCLPdcS9zZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['premise'] = test_df['premise'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")\n",
        "test_df['hypothesis'] = test_df['hypothesis'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 0-9]\", \"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmxFaZN5gDtq",
        "outputId": "37d0fdf2-7cae-48c6-8d41-6869d62b8693"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-1. MODEL 1 : train 원본 + dev 추가 데이터로 학습 (RoBERTa-Large)"
      ],
      "metadata": {
        "id": "fGsPPf2Gg5kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fold_pred_list = fold_training(train_total, test_df, model_name = 'klue/roberta-large')"
      ],
      "metadata": {
        "id": "V5xJweCGgxH4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
        "  if sys.path[0] == '':\n",
        "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
        "  del sys.path[0]\n",
        "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
        "  \n",
        "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
        "  from ipykernel import kernelapp as app\n",
        "Downloading: 100%\n",
        "375/375 [00:00<00:00, 11.3kB/s]\n",
        "Downloading: 100%\n",
        "243k/243k [00:00<00:00, 819kB/s]\n",
        "Downloading: 100%\n",
        "734k/734k [00:00<00:00, 803kB/s]\n",
        "Downloading: 100%\n",
        "173/173 [00:00<00:00, 3.82kB/s]\n",
        "  0%|          | 0/22398 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
        "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
        "  FutureWarning,\n",
        "100%|██████████| 22398/22398 [00:09<00:00, 2338.15it/s]\n",
        "100%|██████████| 5600/5600 [00:01<00:00, 2848.60it/s]\n",
        "100%|██████████| 1666/1666 [00:00<00:00, 5131.96it/s]\n",
        "Downloading: 100%\n",
        "547/547 [00:00<00:00, 16.8kB/s]\n",
        "Downloading: 100%\n",
        "1.25G/1.25G [00:38<00:00, 37.3MB/s]\n",
        "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
        "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3\n",
        "2800/2800 [==============================] - 1495s 517ms/step - loss: 0.7102 - accuracy: 0.6893 - val_loss: 0.4473 - val_accuracy: 0.8214\n",
        "Epoch 2/3\n",
        "2800/2800 [==============================] - 1445s 516ms/step - loss: 0.3668 - accuracy: 0.8651 - val_loss: 0.4061 - val_accuracy: 0.8537\n",
        "Epoch 3/3\n",
        "2800/2800 [==============================] - 1443s 515ms/step - loss: 0.2333 - accuracy: 0.9165 - val_loss: 0.4580 - val_accuracy: 0.8541\n",
        "fold1 training finished!\n",
        "  0%|          | 0/22398 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
        "100%|██████████| 22398/22398 [00:06<00:00, 3373.98it/s]\n",
        "100%|██████████| 5600/5600 [00:02<00:00, 2786.68it/s]\n",
        "100%|██████████| 1666/1666 [00:00<00:00, 4823.59it/s]\n",
        "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
        "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3\n",
        "2800/2800 [==============================] - 1486s 516ms/step - loss: 0.6186 - accuracy: 0.7440 - val_loss: 0.4114 - val_accuracy: 0.8339\n",
        "Epoch 2/3\n",
        "2800/2800 [==============================] - 1441s 515ms/step - loss: 0.3341 - accuracy: 0.8783 - val_loss: 0.3971 - val_accuracy: 0.8525\n",
        "Epoch 3/3\n",
        "2800/2800 [==============================] - 1441s 515ms/step - loss: 0.2050 - accuracy: 0.9263 - val_loss: 0.5276 - val_accuracy: 0.8516\n",
        "fold2 training finished!\n",
        "  0%|          | 0/22398 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
        "100%|██████████| 22398/22398 [00:06<00:00, 3296.55it/s]\n",
        "100%|██████████| 5600/5600 [00:01<00:00, 3280.86it/s]\n",
        "100%|██████████| 1666/1666 [00:00<00:00, 4765.44it/s]\n",
        "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
        "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3\n",
        "2800/2800 [==============================] - 1487s 516ms/step - loss: 0.6405 - accuracy: 0.7366 - val_loss: 0.4754 - val_accuracy: 0.8154\n",
        "Epoch 2/3\n",
        "2800/2800 [==============================] - 1442s 515ms/step - loss: 0.3493 - accuracy: 0.8721 - val_loss: 0.3912 - val_accuracy: 0.8507\n",
        "Epoch 3/3\n",
        "2800/2800 [==============================] - 1442s 515ms/step - loss: 0.2149 - accuracy: 0.9250 - val_loss: 0.4929 - val_accuracy: 0.8446\n",
        "fold3 training finished!\n",
        "  0%|          | 0/22399 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
        "100%|██████████| 22399/22399 [00:06<00:00, 3400.67it/s]\n",
        "100%|██████████| 5599/5599 [00:01<00:00, 3394.85it/s]\n",
        "100%|██████████| 1666/1666 [00:00<00:00, 4748.73it/s]\n",
        "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
        "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3\n",
        "2800/2800 [==============================] - 1516s 527ms/step - loss: 1.1502 - accuracy: 0.3338 - val_loss: 1.1047 - val_accuracy: 0.3415\n",
        "Epoch 2/3\n",
        "2800/2800 [==============================] - 1467s 524ms/step - loss: 1.1288 - accuracy: 0.3368 - val_loss: 1.1084 - val_accuracy: 0.3195\n",
        "Epoch 3/3\n",
        "2800/2800 [==============================] - 1468s 524ms/step - loss: 1.1239 - accuracy: 0.3364 - val_loss: 1.1040 - val_accuracy: 0.3390\n",
        "fold4 training finished!\n",
        "  0%|          | 0/22399 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
        "100%|██████████| 22399/22399 [00:06<00:00, 3429.12it/s]\n",
        "100%|██████████| 5599/5599 [00:01<00:00, 3441.68it/s]\n",
        "100%|██████████| 1666/1666 [00:00<00:00, 5026.51it/s]\n",
        "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
        "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
        "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
        "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
        "Epoch 1/3\n",
        "2800/2800 [==============================] - 1521s 528ms/step - loss: 0.9690 - accuracy: 0.5159 - val_loss: 0.6331 - val_accuracy: 0.7516\n",
        "Epoch 2/3\n",
        "2800/2800 [==============================] - 1471s 525ms/step - loss: 0.5095 - accuracy: 0.8063 - val_loss: 0.4842 - val_accuracy: 0.8198\n",
        "Epoch 3/3\n",
        "2800/2800 [==============================] - 1471s 525ms/step - loss: 0.3144 - accuracy: 0.8879 - val_loss: 0.4804 - val_accuracy: 0.8323\n",
        "fold5 training finished!\n"
      ],
      "metadata": {
        "id": "hHJ5U0VWOr7j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-4. MODEL 4 : 해석 데이터를 추가하지 않은 원본 데이터로 학습 (Pytorch, KoELECTRA)"
      ],
      "metadata": {
        "id": "hiQg4WK3kFKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'monologg/koelectra-base-v3-discriminator'\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "config.num_labels = 3\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
        "\n",
        "train_dataset, eval_dataset = train_test_split(train_total, test_size=0.2, shuffle=True, stratify=train_total['label'], random_state = 2022)\n",
        "\n",
        "tokenized_train = tokenizer(\n",
        "    list(train_dataset['premise']),\n",
        "    list(train_dataset['hypothesis']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128, \n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "tokenized_eval = tokenizer(\n",
        "    list(eval_dataset['premise']),\n",
        "    list(eval_dataset['hypothesis']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")\n",
        "\n",
        "tokenized_test = tokenizer(\n",
        "    list(test_df['premise']),\n",
        "    list(test_df['hypothesis']),\n",
        "    return_tensors=\"pt\",\n",
        "    max_length=128,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    add_special_tokens=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259,
          "referenced_widgets": [
            "65b6169e8a394c54b692bf073c293dee",
            "03a6028b3eb4407b8745c3d6ad341537",
            "5114aa1cf7684b86b8172bd8fefe10a6",
            "15c95b26eeca417bb615fdd4d2011175",
            "758576999865402b9dac1dff50355d95",
            "53bc418340eb4512af707b659b9eda88",
            "3505b7e638bc409a8258e7484cdd6de3",
            "19eccd4cb1ce4f7c9ec9bb1a153bea1d",
            "b7158a302da24c26af382ad4ed002463",
            "1312eb0bd72648bb9c58bc38cf51abc0",
            "1c2354c486184ab0996f8fea9c19b27f",
            "f12e4a1c28dd41d994af09f77271d7b4",
            "a4c2b323036e496592bd8b17e9d6f40e",
            "05fe63c9247b45a4b8d7bd0759d6049e",
            "00ca62bbe9594824bee335062acd16c2",
            "c9a57d3691f1414ebf151cee415e1f42",
            "52356edf065b4d8c8bd7766fca8ad10d",
            "e9cdd5eeb2174aedb9760660ab6030a0",
            "234ce57c4bca4ac6a93a50b68131487a",
            "ce1afec0f1094f6385605f40c744297b",
            "1bebe160a4084a84af249968bf7ea8f9",
            "03a7a949bab946e5bb727489d3d53c24",
            "fde6feb1515846928c8d6633461f212e",
            "35b45c53e70f43de973a22fe0669c317",
            "1c4ffc22ee094a1186d98b164ad114f9",
            "04a4614a63ee4cdebb4c24bbf861c6da",
            "9762ee2ca9694d1ab1d5bd4bc7ba895e",
            "eb23d10154a14fc996232660c73fb4d8",
            "435dd48ab57943409b120d65ffa148d7",
            "7689a0fff5c14eb9af2253cfbb2a12b3",
            "2ccf483ef29044c0a012580cdac8b1f0",
            "59dc052cb80641e2954845e3dac2358d",
            "0f4695c4ec3b451d80c94c3a069a76e2",
            "138fdb3856f1458699af82082e258dad",
            "76f687b8bf1c4065a0ca3205e30f886e",
            "b5e41a8527434e59bb490540ce8927c4",
            "edc78a70b7f0410eac70b8c971c48192",
            "7f4d74e962154917a0e1d18529fce0dc",
            "a41b657259f44b47baccb2abbcba0ed6",
            "1c13da380f1d40d4ba70c6a298a5f7a0",
            "b59ecf486d56463ab772573bad819989",
            "068f21d128f3454a8a100a836d579a80",
            "a3cbb404b467439a884cab7d70919d08",
            "a525b39856154e5b93be805cf32d5c88"
          ]
        },
        "id": "vpwUya18g8-Q",
        "outputId": "8c37ed3a-386c-4b55-9453-a8a3c6c4e96c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65b6169e8a394c54b692bf073c293dee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/61.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f12e4a1c28dd41d994af09f77271d7b4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/467 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fde6feb1515846928c8d6633461f212e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/257k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "138fdb3856f1458699af82082e258dad",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/431M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
            "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pair_dataset, label):\n",
        "        self.pair_dataset = pair_dataset\n",
        "        self.label = label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.pair_dataset.items()}\n",
        "        item['label'] = torch.tensor(self.label[idx])\n",
        "        \n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)"
      ],
      "metadata": {
        "id": "rBZbc3PokHg2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_to_num(label):\n",
        "    label_dict = {\"contradiction\": 0, \"entailment\": 1, \"neutral\": 2, \"answer\": 3}\n",
        "    num_label = []\n",
        "\n",
        "    for v in label:\n",
        "        num_label.append(label_dict[v])\n",
        "    \n",
        "    return num_label\n",
        "train_label = label_to_num(train_dataset['label'].values)\n",
        "eval_label = label_to_num(eval_dataset['label'].values)\n",
        "test_label = label_to_num(test_df['label'].values)\n",
        "\n",
        "train_dataset = BERTDataset(tokenized_train, train_label)\n",
        "eval_dataset = BERTDataset(tokenized_eval, eval_label)\n",
        "test_dataset = BERTDataset(tokenized_test, test_label)"
      ],
      "metadata": {
        "id": "LI4xAeFDtf_8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "  \"\"\" validation을 위한 metrics function \"\"\"\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  probs = pred.predictions\n",
        "\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds) # 리더보드 평가에는 포함되지 않습니다.\n",
        "\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "  }"
      ],
      "metadata": {
        "id": "6D4lCIaIthwy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 초모수 수정하기\n",
        "training_ars = TrainingArguments(\n",
        "    output_dir='./result',\n",
        "    num_train_epochs=7,\n",
        "    per_device_train_batch_size=32, # 64\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.1,\n",
        "    save_total_limit=5,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy='steps',\n",
        "    eval_steps = 500,\n",
        "    load_best_model_at_end = True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_ars,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BQix_fWPtjr2",
        "outputId": "0c2b150e-3bc5-43d8-8708-81b8c52a157e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 22398\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 32\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 4900\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4900' max='4900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4900/4900 29:43, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.531400</td>\n",
              "      <td>0.378810</td>\n",
              "      <td>0.861250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.305200</td>\n",
              "      <td>0.411007</td>\n",
              "      <td>0.856964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.205000</td>\n",
              "      <td>0.547196</td>\n",
              "      <td>0.864107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.123800</td>\n",
              "      <td>0.550713</td>\n",
              "      <td>0.867143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.083900</td>\n",
              "      <td>0.692264</td>\n",
              "      <td>0.871429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.058200</td>\n",
              "      <td>0.701603</td>\n",
              "      <td>0.876786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.042500</td>\n",
              "      <td>0.712986</td>\n",
              "      <td>0.881071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.019600</td>\n",
              "      <td>0.784401</td>\n",
              "      <td>0.882143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.017500</td>\n",
              "      <td>0.811361</td>\n",
              "      <td>0.881429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-500\n",
            "Configuration saved in ./result/checkpoint-500/config.json\n",
            "Model weights saved in ./result/checkpoint-500/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-500/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-1000\n",
            "Configuration saved in ./result/checkpoint-1000/config.json\n",
            "Model weights saved in ./result/checkpoint-1000/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-1000/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-1000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-1500\n",
            "Configuration saved in ./result/checkpoint-1500/config.json\n",
            "Model weights saved in ./result/checkpoint-1500/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-1500/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-1500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-2000\n",
            "Configuration saved in ./result/checkpoint-2000/config.json\n",
            "Model weights saved in ./result/checkpoint-2000/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-2000/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-2000/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-2500\n",
            "Configuration saved in ./result/checkpoint-2500/config.json\n",
            "Model weights saved in ./result/checkpoint-2500/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-2500/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-2500/special_tokens_map.json\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-3000\n",
            "Configuration saved in ./result/checkpoint-3000/config.json\n",
            "Model weights saved in ./result/checkpoint-3000/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-3000/special_tokens_map.json\n",
            "Deleting older checkpoint [result/checkpoint-1000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-3500\n",
            "Configuration saved in ./result/checkpoint-3500/config.json\n",
            "Model weights saved in ./result/checkpoint-3500/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-3500/special_tokens_map.json\n",
            "Deleting older checkpoint [result/checkpoint-1500] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-4000\n",
            "Configuration saved in ./result/checkpoint-4000/config.json\n",
            "Model weights saved in ./result/checkpoint-4000/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-4000/special_tokens_map.json\n",
            "Deleting older checkpoint [result/checkpoint-2000] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 5600\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./result/checkpoint-4500\n",
            "Configuration saved in ./result/checkpoint-4500/config.json\n",
            "Model weights saved in ./result/checkpoint-4500/pytorch_model.bin\n",
            "tokenizer config file saved in ./result/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in ./result/checkpoint-4500/special_tokens_map.json\n",
            "Deleting older checkpoint [result/checkpoint-2500] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./result/checkpoint-500 (score: 0.3788100779056549).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4900, training_loss=0.1423507296309179, metrics={'train_runtime': 1783.6227, 'train_samples_per_second': 87.903, 'train_steps_per_second': 2.747, 'total_flos': 7895986387825032.0, 'train_loss': 0.1423507296309179, 'epoch': 7.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "Tokenizer_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(Tokenizer_NAME)\n",
        "\n",
        "MODEL_NAME = './result/checkpoint-4500'\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "model.resize_token_embeddings(tokenizer.vocab_size)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSDDn8uytlVF",
        "outputId": "6b488d8c-2544-4757-da4c-ad12d9c6055a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/353ae9c3d9daa722551a585351b95934cefcf83155f6ff52a1975fa27863dfe0.9c57bd1e7b894b078a3a8ed91a498ca5fb48334c137fe2ec43e8079db1878f8c\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/896eae0fbbb6c2981e5d4559f5d7fc0deb03e81c292abbe5c89b5bb1cf1294b3.541023ff50f833a9bab3e48e78ae1856cf6744bdb336c86e797eaf675b62b2b8\n",
            "loading file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/tokenizer.json from cache at None\n",
            "loading file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/f2c12b0a8c12c754523b6ed45c1681b4495050720ade30bfe2345e5573d2e141.35f013c4fd3572cfdddbbdf6223ef162dd4fb536bf83007533f201addf3287b7\n",
            "loading configuration file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/353ae9c3d9daa722551a585351b95934cefcf83155f6ff52a1975fa27863dfe0.9c57bd1e7b894b078a3a8ed91a498ca5fb48334c137fe2ec43e8079db1878f8c\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/monologg/koelectra-base-v3-discriminator/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/353ae9c3d9daa722551a585351b95934cefcf83155f6ff52a1975fa27863dfe0.9c57bd1e7b894b078a3a8ed91a498ca5fb48334c137fe2ec43e8079db1878f8c\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"monologg/koelectra-base-v3-discriminator\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForPreTraining\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading configuration file ./result/checkpoint-4500/config.json\n",
            "Model config ElectraConfig {\n",
            "  \"_name_or_path\": \"./result/checkpoint-4500\",\n",
            "  \"architectures\": [\n",
            "    \"ElectraForSequenceClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"embedding_size\": 768,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"electra\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"problem_type\": \"single_label_classification\",\n",
            "  \"summary_activation\": \"gelu\",\n",
            "  \"summary_last_dropout\": 0.1,\n",
            "  \"summary_type\": \"first\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.17.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 35000\n",
            "}\n",
            "\n",
            "loading weights file ./result/checkpoint-4500/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing ElectraForSequenceClassification.\n",
            "\n",
            "All the weights of ElectraForSequenceClassification were initialized from the model checkpoint at ./result/checkpoint-4500.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use ElectraForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ElectraForSequenceClassification(\n",
              "  (electra): ElectraModel(\n",
              "    (embeddings): ElectraEmbeddings(\n",
              "      (word_embeddings): Embedding(35000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): ElectraEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): ElectraLayer(\n",
              "          (attention): ElectraAttention(\n",
              "            (self): ElectraSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): ElectraSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ElectraIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ElectraOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): ElectraClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "output_pred = []\n",
        "output_prob = []\n",
        "\n",
        "for i, data in enumerate(tqdm(dataloader)):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=data['input_ids'].to(device),\n",
        "            attention_mask=data['attention_mask'].to(device),\n",
        "            token_type_ids=data['token_type_ids'].to(device)\n",
        "        )\n",
        "    logits = outputs[0]\n",
        "    prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    result = np.argmax(logits, axis=-1)\n",
        "\n",
        "    output_pred.append(result)\n",
        "    output_prob.append(prob)\n",
        "  \n",
        "pred_answer, output_prob = np.concatenate(output_pred).tolist(), np.concatenate(output_prob, axis=0).tolist()\n",
        "pred_df2 = pd.DataFrame(output_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVVssTBetmtY",
        "outputId": "571434ea-5a24-48d5-b27a-3c368a9dc36e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105/105 [00:04<00:00, 22.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train1_fold1 = fold_pred_list[0]\n",
        "# train1_fold2 = fold_pred_list[1]\n",
        "# train1_fold3 = fold_pred_list[2]\n",
        "# pred_ensemble = (train1_fold1 + train1_fold2 + train1_fold3)/3\n",
        "# label_idx = {'contradiction':0, 'entailment':1, 'neutral':2}\n",
        "# result = [np.argmax(val) for val in np.array(pred_ensemble)]\n",
        "# out = [list(label_idx.keys())[_] for _ in result]\n",
        "# zip_dir = '/content/drive/MyDrive/화요일 스터디 폴더/dacon'\n",
        "# sub_path = zip_dir + \"/data/sample_submission.csv\"\n",
        "# submission = pd.read_csv(sub_path)\n",
        "# submission['label']=out\n",
        "# submission.to_csv(zip_dir + \"/data/ensemble_final.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "ReUefvOptyPY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train1_fold1.to_csv(zip_dir + \"/data/fold1.csv\", index=False)\n",
        "# train1_fold2.to_csv(zip_dir + \"/data/fold2.csv\", index=False)\n",
        "# train1_fold3.to_csv(zip_dir + \"/data/fold3.csv\", index=False)\n",
        "# train1_fold4.to_csv(zip_dir + \"/data/fold4.csv\", index=False)\n",
        "# train1_fold5.to_csv(zip_dir + \"/data/fold5.csv\", index=False)\n",
        "pred_df2.to_csv(zip_dir + \"/data/pred_df2.csv\", index=False)"
      ],
      "metadata": {
        "id": "EozhLS3eIrpx"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train1_fold1 = pd.read_csv(zip_dir + \"/data/fold1.csv\")\n",
        "train1_fold2 = pd.read_csv(zip_dir + \"/data/fold2.csv\")\n",
        "train1_fold3 = pd.read_csv(zip_dir + \"/data/fold3.csv\")\n",
        "pred_df2 = pd.read_csv(zip_dir + \"/data/pred_df2.csv\")"
      ],
      "metadata": {
        "id": "wBLpA9EouUmg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train4 = pred_df2\n",
        "pred_ensemble = (train1_fold1 + train1_fold2 + train1_fold3 + train4)/4\n",
        "label_idx = {'contradiction':0, 'entailment':1, 'neutral':2}\n",
        "result = [np.argmax(val) for val in np.array(pred_ensemble)]\n",
        "out = [list(label_idx.keys())[_] for _ in result]\n",
        "zip_dir = '/content/drive/MyDrive/화요일 스터디 폴더/dacon'\n",
        "sub_path = zip_dir + \"/data/sample_submission.csv\"\n",
        "submission = pd.read_csv(sub_path)\n",
        "submission['label']=out\n",
        "submission.to_csv(zip_dir + \"/data/ensemble_final2.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "jwD3X0RCJ3q9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4JCviFhJWY5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}